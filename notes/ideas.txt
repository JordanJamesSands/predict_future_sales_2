--------------------- TIER 1 ----------------------------------

try this
check week2 advice work i did
	idea:
	notes:
		-this can be tied with test_validation
	result: craeted a python class aggmodel that ...

try this
test_validation:
	idea: iterate costant solutions, compare validation with public leaderboard
	notes:
		-prolly dont bother inclusing constant solutions in validation testing, its just gonna show change over time and the constant predictions are designed to fit the public leaderboard anyway
		-pandas has a 'categorical' column that lightGBM acknowledges by default
		-TAKE A STEP BACK AND THINK ABOUT WHETHER OR NOT THIS IS FRUITFUL
		-would liketo label cat features for a model before passing it to the validation function
			-consider writing a dataprep function for each model and apssing it to validation
		-redo validation split in python
		-SWITCHING TO PYTHON
		-this might be better in python because:
			-it is more obviously OOP
			-it is the language taught in the course
		-use constant predictions, maybe also basic stuff like catboost or target encoding from month before
		-this can be tied with week2 advice, do previous month target encoding
		-this can be tied with probe_leaderboard
	result:

-------------------- TIER 2 --------------------------------------

investigate shifting the submission by a factor

try
lagged / cumulative over time / mean over time feature engineering

try
test validation again
	idea: make sure validation is accurate for more complex models


----------------------TIER 3--------------------------



try
target_encode and XGBoost
	idea: mean encode some basic variable then run XGboost
	notes:
		-i think this was advice
	result:
try
drop old data:
	idea: drop data from a long time ago
	notes: 
		-the model / feature enginering might do this anyway
	result:
try
run catboost after feature engineering

try
look inside catboost:
	idea: get an idea of variable importance and interactions
	notes:
	result:

try
investigate the catboost benchmark, make it validate itself timewise

try
extra precise leaderboard probing?
	idea:
	notes:
		-public leaderboard is approx 35% of data, private is the rest, so getting really specific might not be so fruitful
	result:

-----------------------------DONE-------------------------------------


Done
probe_leaderboard:
	idea: find optimal single value solution, this should be mean of submissions?
	notes: 
		-I did this before, look it up
		-this can be tied in with test validation
	result: 
		-0.2875 is best const sol within reason
		-best is 0.2839853
		-someone on a kaggle forum pointed out that with two constant entries you can compute it analytically, after doing this I found it to be 0.2839853




DONE
colour code vim correctly
	idea: use vim or another editor to code this script with colours
	notes:
		-tried atom and notepad++
		-this is a nightmare
	result: gave up


DONE
validation:
	idea: create a validation sample
	notes: 
		-For N months, select M as initial training set, validated on M+1'th month, then M+=1, repeat 
	result: created validation_split.R

	
DONE
catboost_benchmark:
	idea: run catboost on inital data set to get a benchmark
	notes:
		-catboost likely didnt validate itself timewise
		-saved in gen_data/catboost_submission.csv
		-notebook at eda/scripts/catboost_benchmark.ipynb
	result: scored 1.86768


DONE
remove outliers
	idea: remove outliers
	notes: 
		-the clipping that mimics competition guidelines took care of outliers
	result:done, (i clipped sales values)

DONE
check leak:
	idea: check test subset to see if its accosiated with any other vars
	notes:
		-eda/check_test_sampling.R
		-dataset too large, try aggregating first, did this
	result: no obvious relationship between sales and inclusion of item/shop combo in test set

DONE
aggregate:
	idea: aggregate train data to make it compatible with test
	notes: 
		-need to also clip sales values between [0,20]
	result: aggregation script is preproc/aggregate.R

	
DONE
stitch:
	idea: merge input data
	notes; 
		-drop rows with Nas generated by merging, these contain only items
	result: done, in preproc/stitch.R

