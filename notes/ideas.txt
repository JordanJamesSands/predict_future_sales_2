--------------------- TIER 1 ----------------------------------
try this
test simple validation
	idea: test validation using *only* month 33 as validation
try this
test the pipeline
	idea: erase gen_data, (back it up fisrt) then try to reproduce it

try this
verify data accuracy of data input for modelling
	idea: ensure data was preprocessed correctly, consider using R for a different perspective


try this
formalise a pipeline
	idea: need to get organised
	notes:
		-try this 2 problems in enrich1.ipynb.
			-it sets date_block_num to 33 for test
				-could make full train set an element of pickled train list, but this complicates everything
			-it hardcodes the training list pickle file, what if I change the pipeline?
			
		-create preprocess function that handles train and test/validation ad takes another function to create features
	result


-------------------- TIER 2 --------------------------------------
NOTE: during model1.model1_script1.ipynb irreproducible results found, was gettin rmse between 3 and 4, after changes to enrich, it is usually higher, (could be randomness)


validation split scheme creates validation without item_id or chop_id, contrary to test csv file

think about this
	idea: consider shop item pairs in validation sets might not have existed back then, this could skew things
	notes: calidation is close to test time tho, prolly not a problem



investigate shifting the submission by a factor

try
lagged / cumulative over time / mean over time feature engineering

try
test validation again
	idea: make sure validation is accurate for more complex models


----------------------TIER 3--------------------------


try
test_validation:
	idea: iterate costant solutions, compare validation with public leaderboard
	notes:
		-make a neat noetbook creating validation df
		-this is a nightmare, avoid complicated validation schemes unless they are neccesary
		-validation split is incorrect, y_val_list does not properly mimic test
		-prolly dont bother inclusing constant solutions in validation testing, its just gonna show change over time and the constant predictions are designed to fit the public leaderboard anyway
		-pandas has a 'categorical' column that lightGBM acknowledges by default
		-TAKE A STEP BACK AND THINK ABOUT WHETHER OR NOT THIS IS FRUITFUL
		-would liketo label cat features for a model before passing it to the validation function
			-consider writing a dataprep function for each model and apssing it to validation
		-redo validation split in python
		-SWITCHING TO PYTHON
		-this might be better in python because:
			-it is more obviously OOP
			-it is the language taught in the course
		-use constant predictions, maybe also basic stuff like catboost or target encoding from month before
		-this can be tied with week2 advice, do previous month target encoding
		-this can be tied with probe_leaderboard
	result: JUST GONNA GIVE UP, MAYBE TRY IT LATER, FOR NOW USE MONTH 33 AS VALIDATION

try
target_encode and XGBoost
	idea: mean encode some basic variable then run XGboost
	notes:
		-i think this was advice
	result:
try
drop old data:
	idea: drop data from a long time ago
	notes: 
		-the model / feature enginering might do this anyway
	result:
try
run catboost after feature engineering

try
look inside catboost:
	idea: get an idea of variable importance and interactions
	notes:
	result:

try
investigate the catboost benchmark, make it validate itself timewise

try
extra precise leaderboard probing?
	idea:
	notes:
		-public leaderboard is approx 35% of data, private is the rest, so getting really specific might not be so fruitful
	result:

-----------------------------DONE-------------------------------------

Done
	generate x_valid_33 from train_to_32 so that nans are produced, properly mimicing test
	result: changed enrichment/enrich1.ipynb so that when enriching validation sets, only the relavant training set is used.




Done
check valid_33 
	idea: is x_valid_33 the same as test in simple_validation_split.ipynb?
	result: yes it is, rewrote preproc/simple_validation_split.ipynb accordingly

Done
drop listify_test
	idea:	clean pipeline, (drop listify_test?)
	notes: 	dropped listify, still need to formalise the pipeline
t
Done
reconsider adding test to end of validation list, when I run a sibmission do I wanna use end of list OR go back to start and not partition?
	result: no im gonna carry test through


Done
check week2 advice work i did
	idea:
	notes:
		-this can be tied with test_validation
	result: created a python class aggmodel that takes an arg for how many months to aggregate over, using it in validation testing



Done
probe_leaderboard:
	idea: find optimal single value solution, this should be mean of submissions?
	notes: 
		-I did this before, look it up
		-this can be tied in with test validation
	result: 
		-0.2875 is best const sol within reason
		-best is 0.2839853
		-someone on a kaggle forum pointed out that with two constant entries you can compute it analytically, after doing this I found it to be 0.2839853




DONE
colour code vim correctly
	idea: use vim or another editor to code this script with colours
	notes:
		-tried atom and notepad++
		-this is a nightmare
	result: gave up


DONE
validation:
	idea: create a validation sample
	notes: 
		-For N months, select M as initial training set, validated on M+1'th month, then M+=1, repeat 
	result: created validation_split.R

	
DONE
catboost_benchmark:
	idea: run catboost on inital data set to get a benchmark
	notes:
		-catboost likely didnt validate itself timewise
		-saved in gen_data/catboost_submission.csv
		-notebook at eda/scripts/catboost_benchmark.ipynb
	result: scored 1.86768


DONE
remove outliers
	idea: remove outliers
	notes: 
		-the clipping that mimics competition guidelines took care of outliers
	result:done, (i clipped sales values)

DONE
check leak:
	idea: check test subset to see if its accosiated with any other vars
	notes:
		-eda/check_test_sampling.R
		-dataset too large, try aggregating first, did this
	result: no obvious relationship between sales and inclusion of item/shop combo in test set

DONE
aggregate:
	idea: aggregate train data to make it compatible with test
	notes: 
		-need to also clip sales values between [0,20]
	result: aggregation script is preproc/aggregate.R

	
DONE
stitch:
	idea: merge input data
	notes; 
		-drop rows with Nas generated by merging, these contain only items
	result: done, in preproc/stitch.R

