{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from itertools import product\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = pickle.load(open('../gen_data/x_train--simple_validation_split.ipynb--.pickle','rb'))\n",
    "y_train_list = pickle.load(open('../gen_data/y_train--simple_validation_split.ipynb--.pickle','rb'))\n",
    "x_test_list = pickle.load(open('../gen_data/test_data_enriched--enrich1.ipynb--.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = x_train_list[1]\n",
    "#y_train = y_train_list[1]\n",
    "#test = x_val_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that this data is missing a lot of 0 item sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Target Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train , y_train in zip(x_train_list,y_train_list):\n",
    "    train['item_cnt_month'] = y_train.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_encodings(train):\n",
    "    print(type(train))\n",
    "    item_sales = train.groupby(['date_block_num','item_id'],as_index=False).item_cnt_month.sum()\n",
    "    item_sales.columns = ['date_block_num','item_id','sum_item_sales_back_0']\n",
    "    train = train.merge(item_sales,how='left')\n",
    "        #put this in x_val\n",
    "        #then use OOF or permutations to get data for x_train to avoid overfitting\n",
    "            #maybe just do the basic stuff for x_train, worry about overfitting later\n",
    "\n",
    "    shop_sales = train.groupby(['date_block_num','shop_id'],as_index=False).item_cnt_month.sum()\n",
    "    shop_sales.columns = ['date_block_num','shop_id','sum_shop_sales_back_0']\n",
    "    train = train.merge(shop_sales,how='left')\n",
    "\n",
    "    shop_item_sales = train.groupby(['date_block_num','shop_id','item_id'],as_index=False).agg({'item_cnt_month':'sum'})\n",
    "    shop_item_sales.columns = ['date_block_num','shop_id','item_id','item_cnt_month_back_0']\n",
    "    train = train.merge(shop_item_sales,how='left',on=['date_block_num','shop_id','item_id'])\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "x_train_list = list(map(create_target_encodings,x_train_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate 0 Entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to include entries where 0 sales were made for item/shop pairs in a month.\n",
    "So this doesnt get out of hand, gonna focus only on all possible item/shop pairs based on sales in that month, \n",
    "this is what the coursera course did, see outside/Programming_assignment_week_4.ipynb for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gen_zeros(train):\n",
    "    months = range(train.date_block_num.min(),train.date_block_num.max()+1)\n",
    "    to_pandas=[]\n",
    "    print('Computing for month:',end=' ')\n",
    "    for month in months:\n",
    "        print(month,end=', ')\n",
    "        subtrain = train[train.date_block_num==month].copy()\n",
    "        all_shops = subtrain.shop_id.unique()\n",
    "        all_items = subtrain.item_id.unique()\n",
    "\n",
    "        pairs = product(all_shops,all_items)\n",
    "        to_pandas.append([(month,x[0],x[1]) for x in pairs])\n",
    "\n",
    "    train_filled = pd.DataFrame(np.vstack(to_pandas),columns=['date_block_num','shop_id','item_id'])\n",
    "    #train_filled['shop_id'] = train_filled.shop_id.astype(np.int32)\n",
    "    #train_filled['item_id'] = train_filled.item_id.astype(np.int32)\n",
    "    \n",
    "    train_filled = train_filled.merge(train,how='left',on=['date_block_num','shop_id','item_id'])\n",
    "    #entries where item_cnt_month is supposed to be 0 are now created as NA\n",
    "    train_filled.fillna(0,inplace=True)\n",
    "    \n",
    "    return train_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for month: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, Computing for month: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, "
     ]
    }
   ],
   "source": [
    "x_train_list = list(map(gen_zeros,x_train_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Lag Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to downcast data types to 32 bits\n",
    "def downcast(df):\n",
    "    float_cols = [col for col in df if df[col].dtype=='float64']\n",
    "    int_cols = [col for col in df if df[col].dtype=='int64']\n",
    "\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols] = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_train(train_filled):\n",
    "    #gonna iterively copy a subset of the data, rename the date block and aome other cols then merge it back in dateback_gen = range(1,13)\n",
    "    dateback_gen = range(1,13)\n",
    "    lag_train_filled = downcast(train_filled)\n",
    "    del train_filled\n",
    "    print('dateback=',end=' ')\n",
    "    gc.collect()\n",
    "    for dateback in dateback_gen: \n",
    "        #this line inneficient?\n",
    "        to_shift = lag_train_filled[['date_block_num','shop_id','item_id','sum_item_sales_back_0','sum_shop_sales_back_0','item_cnt_month_back_0']].copy()\n",
    "        to_shift['date_block_num'] = to_shift.date_block_num + dateback\n",
    "        newcols = ['date_block_num','shop_id','item_id','sum_item_sales_back_'+str(dateback),'sum_shop_sales_back_'+str(dateback),'item_cnt_month_back_'+str(dateback)]\n",
    "        to_shift.columns = newcols\n",
    "        #print(newcols)\n",
    "        print(dateback,end=', ')\n",
    "        lag_train_filled = lag_train_filled.merge(to_shift,on=['date_block_num','shop_id','item_id'],how='left').fillna(0)\n",
    "        del to_shift\n",
    "        gc.collect()\n",
    "    \n",
    "    #remove first 12 months\n",
    "    lag_train_filled = lag_train_filled[lag_train_filled.date_block_num>=12]\n",
    "    return lag_train_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dateback= 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, dateback= 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, "
     ]
    }
   ],
   "source": [
    "x_train_list = list(map(create_lag_train,x_train_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6425094, 47)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_list[1][x_train_list[1].date_block_num>=12].shape\n",
    "#This should have 6425094 to mimmick the course script (except this is using all of the shops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularise Training Target encoding for\n",
    "# --//--\\_back\\_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mytest(tup):\n",
    "    return tup[0]+tup[1]\n",
    "mytest((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_test(tup):\n",
    "    train = tup[0]\n",
    "    test = tup[1]\n",
    "    test = downcast(test)\n",
    "    ref_date_block = test.date_block_num.mean()\n",
    "    dateback_gen = range(1,13)\n",
    "    for dateback in dateback_gen:\n",
    "        print('Getting information from '+str(ref_date_block - dateback))\n",
    "        hist_data = train[train.date_block_num==ref_date_block - dateback][['date_block_num','shop_id','item_id','sum_item_sales_back_0','sum_shop_sales_back_0','item_cnt_month_back_0']]\n",
    "        hist_data.date_block_num = ref_date_block\n",
    "        hist_data.columns = ['date_block_num','shop_id','item_id','sum_item_sales_back_'+str(dateback),'sum_shop_sales_back_'+str(dateback),'item_cnt_month_back_'+str(dateback)]\n",
    "        test = test.merge(hist_data,how='left').fillna(0)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting information from 32.0\n",
      "Getting information from 31.0\n",
      "Getting information from 30.0\n",
      "Getting information from 29.0\n",
      "Getting information from 28.0\n",
      "Getting information from 27.0\n",
      "Getting information from 26.0\n",
      "Getting information from 25.0\n",
      "Getting information from 24.0\n",
      "Getting information from 23.0\n",
      "Getting information from 22.0\n",
      "Getting information from 21.0\n",
      "Getting information from 33.0\n",
      "Getting information from 32.0\n",
      "Getting information from 31.0\n",
      "Getting information from 30.0\n",
      "Getting information from 29.0\n",
      "Getting information from 28.0\n",
      "Getting information from 27.0\n",
      "Getting information from 26.0\n",
      "Getting information from 25.0\n",
      "Getting information from 24.0\n",
      "Getting information from 23.0\n",
      "Getting information from 22.0\n"
     ]
    }
   ],
   "source": [
    "x_test_list = list(map(create_lag_test,zip(x_train_list,x_test_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if something in train didnt have a item cat id, (or another variable)? would the merge miss it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_train(train):\n",
    "    sum_item_sales_back_0 = train.sum_item_sales_back_0\n",
    "    sum_shop_sales_back_0 = train.sum_shop_sales_back_0\n",
    "    item_cnt_month_back_0 = train.item_cnt_month_back_0\n",
    "\n",
    "    train = train.drop(['sum_item_sales_back_0','sum_shop_sales_back_0','item_cnt_month_back_0','item_cnt_month'],axis=1)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = list(map(clear_train,x_train_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ce1127d81494>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../gen_data/x_train--features1.ipynb--.pickle'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../gen_data/x_test--features1.ipynb--.pickle'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "pickle.dump(x_train_list,open('../gen_data/x_train--features1.ipynb--.pickle','wb'))\n",
    "pickle.dump(x_test_list,open('../gen_data/x_test--features1.ipynb--.pickle','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
